{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Preprocessing Raw Data**"
      ],
      "metadata": {
        "id": "9P8Q4DzNZecd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pO0C6PA2YzBA"
      },
      "outputs": [],
      "source": [
        "#!pip install spacy\n",
        "#!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "mX9zARPC2vJ5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "M3fwVqICZ1Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"jobs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    jobs_data = json.load(f)\n",
        "\n",
        "cleaned_jobs = []\n",
        "\n",
        "for job in jobs_data:\n",
        "    title = job.get(\"title\", \"\")\n",
        "    keyword = job.get(\"keyword\", \"\")\n",
        "    raw_text = job.get(\"description\", \"\")\n",
        "\n",
        "    text = raw_text.replace(\"\\n\", \" \")                           #Remove line breaks\n",
        "    text = re.sub(r\"http\\S+|www\\S+|[\\[\\]{}()]\", \"\", text)        #Remove URLs and brackets\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()                  #Normalize spaces\n",
        "\n",
        "    #sentence splitting\n",
        "    doc = nlp(text)\n",
        "    raw_sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 15]\n",
        "\n",
        "    #cleaning each sentence\n",
        "    cleaned_sentences = []\n",
        "    for sent in raw_sentences:\n",
        "        cleaned = re.sub(r\"[^A-Za-z0-9\\s.,:/\\-]\", \"\", sent)\n",
        "        cleaned = re.sub(r\"\\s{2,}\", \" \", cleaned).strip()\n",
        "        cleaned_sentences.append(cleaned)\n",
        "\n",
        "    cleaned_jobs.append({\n",
        "        \"title\": title,\n",
        "        \"keyword\": keyword,\n",
        "        \"sentences\": cleaned_sentences\n",
        "    })\n",
        "\n",
        "with open(\"cleaned_jobs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_jobs, f, indent=2)"
      ],
      "metadata": {
        "id": "xTMEvjDQY99L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Phrase Extraction (Skills)**"
      ],
      "metadata": {
        "id": "RLIZ_0rygh70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"final_categorized_phrases.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_phrases = json.load(f)\n",
        "\n",
        "categorized_phrases = {\n",
        "    \"SKILL\": raw_phrases.get(\"SKILL_PHRASES\", []),\n",
        "    \"TOOL\": raw_phrases.get(\"TOOL_PHRASES\", []),\n",
        "    \"SOFT_SKILL\": raw_phrases.get(\"SOFT_SKILL_PHRASES\", []),\n",
        "    \"FIELD\": raw_phrases.get(\"FIELD_PHRASES\", []),\n",
        "    \"LANG\": raw_phrases.get(\"LANG_PHRASES\", []),\n",
        "    \"CERT\": raw_phrases.get(\"CERT_PHRASES\", [])\n",
        "}"
      ],
      "metadata": {
        "id": "omYLrC2Iiq6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cleaned_jobs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    job_data = json.load(f)\n",
        "\n",
        "def is_phrase_in_text(phrase, text):\n",
        "    phrase_clean = phrase.strip().lower()\n",
        "    text_clean = text.lower()\n",
        "    if len(phrase_clean) <= 3:\n",
        "        pattern = r'(?<!\\w)' + re.escape(phrase_clean) + r'(?!\\w)'\n",
        "    else:\n",
        "        pattern = r'\\b' + re.escape(phrase_clean) + r'\\b'\n",
        "    return re.search(pattern, text_clean) is not None\n",
        "\n",
        "\n",
        "results = []\n",
        "for job in tqdm(job_data, desc=\"Processing jobs\"):\n",
        "    job_text = \" \".join(job.get(\"sentences\", []))\n",
        "    matched_entities = []\n",
        "\n",
        "    for category, phrases in categorized_phrases.items():\n",
        "        for phrase in phrases:\n",
        "            if is_phrase_in_text(phrase, job_text):\n",
        "                matched_entities.append({\n",
        "                    \"text\": phrase,\n",
        "                    \"label\": category\n",
        "                })\n",
        "\n",
        "    results.append({\n",
        "        \"title\": job.get(\"title\", \"\"),\n",
        "        \"text\": job_text,\n",
        "        \"entities\": matched_entities\n",
        "    })\n",
        "\n",
        "with open(\"extracted_phrases.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Done, extracted all releevant phrases to extracted_phrases.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evZiRDnBMp6U",
        "outputId": "6b0a08ff-f14f-4bb0-aecc-569327bfe76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing jobs: 100%|██████████| 6697/6697 [34:32<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done, extracted all releevant phrases to extracted_phrases.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. BIO-TAGGING**"
      ],
      "metadata": {
        "id": "vbli2T1Ax_XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cleaned_jobs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    all_jobs = json.load(f)\n",
        "\n",
        "with open(\"extracted_phrases.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    phrase_data = json.load(f)\n",
        "\n",
        "phrase_lookup = {entry[\"title\"]: entry[\"entities\"] for entry in phrase_data}\n",
        "\n",
        "def is_phrase_in_text(phrase, text):\n",
        "    phrase = phrase.strip().lower()\n",
        "    text = text.lower()\n",
        "    if len(phrase) <= 3:\n",
        "        pattern = r'(?<!\\w)' + re.escape(phrase) + r'(?!\\w)'\n",
        "    else:\n",
        "        pattern = r'\\b' + re.escape(phrase) + r'\\b'\n",
        "    return re.search(pattern, text) is not None\n",
        "\n",
        "def tag_sentence(text, matched_phrases):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    token_lowers = [t.lower() for t in tokens]\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    for ent in matched_phrases:\n",
        "        phrase = ent[\"text\"].lower()\n",
        "        label = ent[\"label\"]\n",
        "        phrase_tokens = phrase.split()\n",
        "        n = len(phrase_tokens)\n",
        "\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            if token_lowers[i:i + n] == phrase_tokens:\n",
        "                labels[i] = f\"B-{label}\"\n",
        "                for j in range(1, n):\n",
        "                    labels[i + j] = f\"I-{label}\"\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"labels\": labels\n",
        "    }"
      ],
      "metadata": {
        "id": "bhxzjeNcTdi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_jobs = []\n",
        "for job in all_jobs:\n",
        "    job_title = job[\"title\"]\n",
        "    keyword = job[\"keyword\"]\n",
        "    sentences = job[\"sentences\"]\n",
        "    matched_entities = phrase_lookup.get(job_title, [])\n",
        "\n",
        "    tagged_sents = []\n",
        "    for sent in sentences:\n",
        "        matched = [e for e in matched_entities if is_phrase_in_text(e[\"text\"], sent)]\n",
        "        tagged_sents.append(tag_sentence(sent, matched))\n",
        "\n",
        "    tagged_jobs.append({\n",
        "        \"title\": job_title,\n",
        "        \"keyword\": keyword,\n",
        "        \"tagged_sentences\": tagged_sents\n",
        "    })\n",
        "\n",
        "with open(\"bio_tagged_all_jobs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tagged_jobs, f, indent=2)"
      ],
      "metadata": {
        "id": "Ndrg6HpkjaPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}